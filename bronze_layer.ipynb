{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "274ff608-0153-4be7-b01a-1d14df2aed0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mount\n",
    "import json\n",
    "# Set up the configurations for mounting the GCS bucket\n",
    "gcs_bucket_name = \"dybucket\"\n",
    "mount_point = \"/mnt/dy\"\n",
    "project_id = \"mentorsko-1725955324975\"\n",
    "service_account_key = \"/dbfs/FileStore/tables/mentorsko_1725955324975_032294f8805c.json\"\n",
    "\n",
    "# Read the service account key file\n",
    "with open(service_account_key, 'r') as key_file:\n",
    "    service_account_info = json.load(key_file)\n",
    "\n",
    "# Define the GCS service account credentials\n",
    "config = {\n",
    "    \"fs.gs.impl\": \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\",\n",
    "    \"fs.gs.auth.service.account.enable\": \"true\",\n",
    "    \"fs.gs.auth.service.account.email\": service_account_info[\"client_email\"],\n",
    "    \"fs.gs.auth.service.account.private.key.id\": service_account_info[\"private_key_id\"],\n",
    "    \"fs.gs.auth.service.account.private.key\": service_account_info[\"private_key\"],\n",
    "    \"fs.gs.project.id\": project_id\n",
    "}\n",
    "\n",
    "# Mount the GCS bucket\n",
    "dbutils.fs.mount(\n",
    "    source=f\"gs://{gcs_bucket_name}\",\n",
    "    mount_point=mount_point,\n",
    "    extra_configs=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d816fe7d-6702-4820-a46b-ed5b8f5b6d74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/mnt/dy/dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21733d93-b5e7-4851-b109-d5fb8d8ba173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create schema if not exists rohit_databricks_npmentorskool_onmicrosoft_com.bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "244cff78-0f0e-49d8-a197-ae889f203877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The source directory containing your CSV files\n",
    "source_path = '/mnt/dy/dataset'\n",
    "\n",
    "# The destination catalog and schema where you want to create the new tables\n",
    "catalog = 'rohit_databricks_npmentorskool_onmicrosoft_com'\n",
    "schema = 'bronze'\n",
    "\n",
    "# Get a list of files in the source directory\n",
    "try:\n",
    "    files = dbutils.fs.ls(source_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing source path {source_path}: {e}\")\n",
    "    # Stop execution if the source directory is not accessible\n",
    "    dbutils.notebook.exit(\"Stopping notebook execution due to inaccessible source path.\")\n",
    "\n",
    "# Loop through each file\n",
    "for file_info in files:\n",
    "    file_path = file_info.path\n",
    "    file_name = file_info.name\n",
    "\n",
    "    # Check if the file is a CSV file to avoid processing subdirectories\n",
    "    if file_name.endswith('.csv'):\n",
    "        # Create a table name from the file name by removing the '.csv' extension\n",
    "        # and ensuring it's a valid table name (e.g., replacing spaces)\n",
    "        table_name = file_name.replace('.csv', '').replace(' ', '_')\n",
    "        \n",
    "        # Build the full destination table path\n",
    "        destination_table = f\"{catalog}.{schema}.{table_name}\"\n",
    "        \n",
    "        print(f\"Processing file: {file_name}\")\n",
    "\n",
    "        try:\n",
    "            # Step 1: Read the CSV file into a Spark DataFrame\n",
    "            # The schema, including column names with special characters, is inferred here.\n",
    "            df = spark.read.format('csv').options(\n",
    "                header='true',\n",
    "                inferSchema='true',\n",
    "                timestampFormat='dd-MM-yyyy HH.mm'\n",
    "            ).load(file_path)\n",
    "\n",
    "            # Step 2: Write the DataFrame to a Delta table with column mapping enabled\n",
    "            # This single operation creates the table and loads the data.\n",
    "            print(f\"Writing data to Delta table: {destination_table} with column mapping enabled.\")\n",
    "            df.write.format('delta').mode('overwrite').option(\n",
    "                \"delta.columnMapping.mode\", \"name\"\n",
    "            ).option(\"mergeSchema\", \"true\").saveAsTable(destination_table)\n",
    "            \n",
    "            print(f\"Successfully created and loaded data into table: {destination_table}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "783355df-abed-493c-b2d6-5ffd1bc8c8d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM rohit_databricks_npmentorskool_onmicrosoft_com.bronze.addresses LIMIT 100;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6693275156728344,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
